#!/bin/bash

# ================= é…ç½®ä¿®æ”¹åŒº =================
# ðŸ”´ æ•°æ®é›†æ‰€åœ¨è·¯å¾„
DATA_DIR="code/data" 
# ===========================================

RESULT_DIR="experiment_results"
CSV_FILE="$RESULT_DIR/metrics.csv"
mkdir -p $RESULT_DIR/logs

# åˆå§‹åŒ– CSV è¡¨å¤´
echo "algorithm,dataset,job_id,total_execution_time,avg_iteration_time,network_communication,peak_memory_usage,iterations_to_converge,convergence_rate,cpu_utilization" > $CSV_FILE

# å‡†å¤‡ Jar åŒ…çŽ¯å¢ƒ
export HADOOP_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath):$(pwd)/pagerank-compare.jar
export GIRAPH_JAR=/usr/local/hadoop/share/hadoop/yarn/lib/giraph.jar

# å®šä¹‰æ•°æ®é›† (Name : FileName)
declare -A DATASETS
DATASETS=( ["small"]="cit-HepPh.txt" ["medium"]="twitter_combined.txt" ["large"]="web-BerkStan.txt" )

# ================= 1. é¢„å¤„ç†æ•°æ® =================
echo ">>> Step 1: Preprocessing Data (Java)..."
for name in "${!DATASETS[@]}"; do
    raw_file="$DATA_DIR/${DATASETS[$name]}"
    
    # æ£€æŸ¥æœ¬åœ°æºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if [ -f "$raw_file" ]; then
        echo "Processing $name ($raw_file)..."
        # è°ƒç”¨ Java å·¥å…·è½¬æ¢æ•°æ®æ ¼å¼
        java -cp classes DataPreprocess "$raw_file" "${name}_mr.txt" "${name}_giraph.txt"
        
        # 1. ç¡®ä¿ç›®å½•å­˜åœ¨
        hdfs dfs -mkdir -p /user/root/input/$name
        
        # 2. ðŸ”´ ä¿®æ”¹å¤„ï¼šå…ˆå¼ºåˆ¶åˆ é™¤æ—§æ–‡ä»¶ (é˜²æ­¢è¦†ç›–å¤±è´¥æˆ–æ®‹ç•™)
        echo "    Cleaning up old files in HDFS..."
        hdfs dfs -rm -r -f /user/root/input/$name/mr.txt
        hdfs dfs -rm -r -f /user/root/input/$name/giraph.txt

        # 3. ä¸Šä¼ æ–°æ–‡ä»¶
        echo "    Uploading new files..."
        hdfs dfs -put "${name}_mr.txt" /user/root/input/$name/mr.txt
        hdfs dfs -put "${name}_giraph.txt" /user/root/input/$name/giraph.txt
    else
        echo "âš ï¸ Warning: Local file $raw_file not found, skipping..."
    fi
done

# ================= 2. å¼€å§‹å®žéªŒå¾ªçŽ¯ =================
# é¡ºåº: Small -> Medium -> Large
for name in "${!DATASETS[@]}"; do
    # å†æ¬¡æ£€æŸ¥ HDFS æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™è·³è¿‡è¯¥æ•°æ®é›†
    hdfs dfs -test -e /user/root/input/$name/mr.txt
    if [ $? -ne 0 ]; then 
        echo "âŒ HDFS data for $name missing, skipping experiment."
        continue
    fi

    for algo in "giraph" "mapreduce"; do
        # æ¯ä¸ªå®žéªŒé‡å¤ 3 æ¬¡
        for run in {1..3}; do
            echo "=========================================================="
            echo "Running $algo on $name (Run $run/3)"
            echo "=========================================================="

            # --- A. é‡å¯é›†ç¾¤ (ç¡®ä¿å†…å­˜æ¸…ç†å¹²å‡€ï¼Œå®žéªŒçŽ¯å¢ƒçº¯å‡€) ---
            echo ">>> Restarting Cluster..."
            stop-all.sh > /dev/null 2>&1
            sleep 5
            start-all.sh > /dev/null 2>&1
            sleep 10
            hdfs dfsadmin -safemode leave > /dev/null 2>&1

            # --- B. å‡†å¤‡æ—¥å¿—å’Œè¾“å‡ºè·¯å¾„ ---
            JOB_ID="${algo}_${name}_${run}"
            LOG="$RESULT_DIR/logs/${JOB_ID}.log"
            MON="$RESULT_DIR/logs/${JOB_ID}_mon.log"
            
            # åˆ é™¤æ—§çš„è¾“å‡ºç›®å½• (æ­¤å¤„ä¹Ÿä¿ç•™äº†æ¸…ç†é€»è¾‘)
            hdfs dfs -rm -r /user/root/output/$JOB_ID > /dev/null 2>&1
            
            # --- C. å¯åŠ¨ç›‘æŽ§ (å…³é”®: -n DEV ç”¨äºŽæŠ“å–ç½‘ç»œæµé‡) ---
            sar -u -r -n DEV 1 > $MON 2>&1 &
            MON_PID=$!

            start_ts=$(date +%s)

            # --- D. æ‰§è¡Œä»»åŠ¡ ---
            if [ "$algo" == "giraph" ]; then
                hadoop jar $GIRAPH_JAR \
                    org.apache.giraph.GiraphRunner \
                    PageRankGiraph \
                    -mc PageRankMasterCompute \
                    -vif org.apache.giraph.io.formats.JsonLongDoubleFloatDoubleVertexInputFormat \
                    -vip /user/root/input/$name/giraph.txt \
                    -vof org.apache.giraph.io.formats.IdWithValueTextOutputFormat \
                    -op /user/root/output/$JOB_ID \
                    -w 1 -ca giraph.SplitMasterWorker=false > $LOG 2>&1
            else
                hadoop jar pagerank-compare.jar \
                    PageRankMapReduce \
                    /user/root/input/$name/mr.txt \
                    /user/root/output/$JOB_ID > $LOG 2>&1
            fi

            end_ts=$(date +%s)
            duration=$((end_ts - start_ts))
            
            # åœæ­¢ç›‘æŽ§
            kill $MON_PID

            # --- E. æŠ“å–ä»ŽèŠ‚ç‚¹æ—¥å¿— (å…³é”®æ­¥éª¤) ---
            # MapReduce ä¸éœ€è¦è¿™ä¸€æ­¥ï¼Œå› ä¸ºå®ƒæ‰“å°åœ¨ Driver ä¸Š
            # Giraph éœ€è¦åŽ» Slave èŠ‚ç‚¹æŠ“å– finishSuperstep æ—¥å¿—
            if [ "$algo" == "giraph" ]; then
                echo ">>> Hunting for Giraph logs on Slave nodes..."
                
                # ä»Žä¸»æ—¥å¿—ä¸­æå– Application ID
                APP_ID=$(grep -o "application_[0-9_]*" $LOG | head -1)
                
                if [ -n "$APP_ID" ]; then
                    # éåŽ†æ‰€æœ‰èŠ‚ç‚¹ (Master, Slave1, Slave2)
                    for node in master slave1 slave2; do
                        echo "    Fetching from $node..."
                        # è¿œç¨‹æ‰§è¡Œ grepï¼ŒæŸ¥æ‰¾ finishSuperstepï¼Œå¹¶å°†ç»“æžœè¿½åŠ åˆ°æœ¬åœ° LOG æ–‡ä»¶
                        # 2>/dev/null ç”¨äºŽå±è”½ SSH çš„è¿žæŽ¥è­¦å‘Šä¿¡æ¯
                        ssh $node "grep -r 'finishSuperstep' /usr/local/hadoop/logs/userlogs/$APP_ID 2>/dev/null" >> $LOG
                    done
                else
                    echo "    âš ï¸ Warning: Could not find Application ID, skipping log fetch."
                fi
            fi

            # --- F. Javaè§£æžç»“æžœå¹¶å†™å…¥CSV ---
            echo ">>> Parsing results..."
            java -cp classes LogAnalyzer $algo $name $JOB_ID $LOG $MON $duration >> $CSV_FILE
            
            echo ">>> Finished in ${duration}s"
        done
    done
done

echo "âœ… All Done! Check $CSV_FILE"